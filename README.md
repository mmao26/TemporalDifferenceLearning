## Temporal Difference Learning

Temporal difference (TD) methods are a class of incremental learning procedures specialized for prediction problems. Whereas conventional prediction-learning methods are driven by the error between predicted and actual outcomes. TD methods are similarly driven by the error or difference between temporally successive predictions; with them, learning occurs whenever there is a change in prediction over time.

In this project, I simulated TD(λ) with different weights update rules, which update weights (1) after each training set and (2) after each sequence. I compare our results about Root Mean Square Error (RMSE) with results from Sutton’s paper [1]. I also show how hyper-parameters like α (learning rate) and convergence criterion have effect on RMSE. Different curves for different hyper-parameters are also provided.


### Running the Tests
All results for each project are shown in **Report.pdf**.

### Authors
* **Manqing Mao,** maomanqing@gmail.com

<!-- See also the list of [contributors](https://github.com/your/project/contributors) who participated in this project. -->
